{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f32a2dd-a158-49ca-a31d-805a3fd622e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_file_path = \"dog-and-cat.zip\"\n",
    "DATA_PATH = \"dog-and-cat\"\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a598e46-02b7-45ab-98a7-cb749a27d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import makedirs, listdir\n",
    "from shutil import copyfile\n",
    "from random import seed, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb3e9b7-9b30-41df-990a-d5bbc0dfc813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data\n",
    "# modify the below path if needed\n",
    "train_folder = \"./dog-and-cat/dog-and-cat/training_set\"\n",
    "test_folder = \"./dog-and-cat/dog-and-cat/test_set\"\n",
    "train_dataset = ImageFolder(train_folder)\n",
    "test_dataset = ImageFolder(test_folder)\n",
    "# %%\n",
    "# check data\n",
    "train_file_names = []\n",
    "train_labels = []\n",
    "test_file_names = []\n",
    "test_labels = []\n",
    "\n",
    "for label in os.listdir(train_folder):\n",
    "    label_folder = os.path.join(train_folder, label)\n",
    "    if os.path.isdir(label_folder):\n",
    "        for file in os.listdir(label_folder):\n",
    "            if file.endswith('.jpg'):\n",
    "                train_file_names.append(file)\n",
    "                train_labels.append(file[0:3])\n",
    "\n",
    "for label in os.listdir(test_folder):\n",
    "    label_folder = os.path.join(test_folder, label)\n",
    "    if os.path.isdir(label_folder):\n",
    "        for file in os.listdir(label_folder):\n",
    "            if file.endswith('.jpg'):\n",
    "                test_file_names.append(file)\n",
    "                test_labels.append(file[0:3])\n",
    "\n",
    "training_data = {'id': train_file_names, 'label': train_labels, 'split': \"train\"}\n",
    "testing_data = {'id': test_file_names, 'label': test_labels, 'split': \"test\"}\n",
    "\n",
    "training_df = pd.DataFrame(training_data)\n",
    "testing_df = pd.DataFrame(testing_data)\n",
    "\n",
    "df = pd.concat([training_df, testing_df], ignore_index=True)\n",
    "# split val and train 1:9\n",
    "train_df, val_df = train_test_split(training_df, test_size=0.2, random_state=2023)\n",
    "train_df = train_df.iloc[:, :2]\n",
    "val_df = val_df.iloc[:, :2]\n",
    "test_df = testing_df.iloc[:, :2]\n",
    "\n",
    "print('The shape of train data',train_df.shape)\n",
    "print('The shape of test data',test_df.shape)\n",
    "print('The shape of val data',val_df.shape)\n",
    "print('The shape of all data', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2da79b9-f1bd-4535-8bc6-507b0bc494e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build torch dataset\n",
    "class DogCatLoader(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = self.checkChannel(\n",
    "            dataset\n",
    "        )  # some images are CMYK, Grayscale, check only RGB\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image = Image.open(self.dataset[item][0])\n",
    "        classCategory = self.dataset[item][1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, classCategory\n",
    "\n",
    "    # 非 RGB 圖片會被忽略\n",
    "    def checkChannel(self, dataset):\n",
    "        datasetRGB = []\n",
    "        for index in range(len(dataset)):\n",
    "            if Image.open(dataset[index][0]).getbands() == (\n",
    "                \"R\",\n",
    "                \"G\",\n",
    "                \"B\",\n",
    "            ):  # Check Channels\n",
    "                datasetRGB.append(dataset[index])\n",
    "        return datasetRGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2272bd4-bb30-49fb-a5fb-70f06f432019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit here if needed\n",
    "image_size = 224\n",
    "image_channel = 3 # RGB\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5371073d-c6bd-4d33-8cd9-7337dbd90bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), shear=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) params from Imagenet\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) params from Imagenet\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b73d588-b43f-49f2-ba27-687b97cd7ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將訓練資料再切分為 Train、Validation\n",
    "training_set = ImageFolder(train_folder)\n",
    "\n",
    "train_data, valid_data, train_label, valid_label = train_test_split(\n",
    "    training_set.imgs, training_set.targets, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = DogCatLoader(train_data, train_transform)\n",
    "valid_dataset = DogCatLoader(valid_data, valid_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da21a30-2c45-4a5c-aa87-caa304b79599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting GPU\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "print(f\"train on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa5086-ebdf-4053-b493-7e9af5fbe386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, image_size, image_channel):\n",
    "        super().__init__()\n",
    "        # input_size: 3*224*224\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(image_channel, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(0.2),\n",
    "            ) # 32*112*112\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(0.2),\n",
    "            ) # 64*56*56\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(0.2), \n",
    "            ) # 128*28*28\n",
    "        \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(0.2)\n",
    "            ) # 256*14*14\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(256*(image_size//16)*(image_size//16), 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            )\n",
    "\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2(output)\n",
    "        output = self.conv3(output)\n",
    "        output = self.conv4(output)\n",
    "        output = output.view(-1, 256*(image_size//16)*(image_size//16)) # flatten\n",
    "        output = self.fc1(output)\n",
    "        output = self.fc2(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f90b67-b2a8-4db8-8c5a-f5803d8081cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop def\n",
    "from tqdm import tqdm\n",
    "def training_loop(n_epochs, optimizer, scheduler, model, loss_fn, train_loader, val_loader, patience = 5):\n",
    "    best_val_accuracy = 0.0  \n",
    "    early_stopping_patience = patience\n",
    "    no_improvement_counter = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_train = 0.0\n",
    "        with tqdm(train_loader, desc=f'Epoch {epoch+1}/{n_epochs}', unit='batch') as t:\n",
    "            for images, labels in t:\n",
    "                images = images.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "                outputs = model(images)  # one batch at a time\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_train += loss.item()\n",
    "\n",
    "                val_accuracy, val_loss, train_accuracy = validate(model, train_loader, val_loader, loss_fn)\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                scheduler.step(val_loss)\n",
    "\n",
    "        if epoch == 0 or (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch {epoch}, loss: {loss_train/(len(train_loader))}, acc: {train_accuracy}, val_loss: {val_loss/(len(val_loader))}, val_acc: {val_accuracy}, lr:{current_lr}')\n",
    "        \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "        else: \n",
    "            no_improvement_counter += 1\n",
    "\n",
    "        if no_improvement_counter >= early_stopping_patience:\n",
    "            print(f\"Early stopping after {epoch+1} epochs.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc48a40-8a28-46d2-8cb2-0d704de3be20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation def\n",
    "def validate(model, train_loader, val_loader, loss_fn, testing = False):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        if name == \"train\" and testing == True:\n",
    "            train_accuracy = None\n",
    "            continue\n",
    "        else:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            total_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for imgs, labels in loader:\n",
    "                    outputs = model(imgs)\n",
    "                    _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "                    total += labels.shape[0]\n",
    "                    correct += int((predicted == labels).sum())\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "            if name == \"val\":\n",
    "                val_accuracy = correct / total\n",
    "                val_loss = val_loss\n",
    "            \n",
    "            if name == \"train\":\n",
    "                train_accuracy = correct / total\n",
    "    \n",
    "    return val_accuracy, val_loss, train_accuracy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6132541-d9fb-4c81-b0a5-9ad3f3203de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageClassifier(2, image_size, image_channel).to(device=device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.5, verbose=True, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ef62a-460c-4b2e-8344-fd2bfd3d8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strat training\n",
    "model.train()\n",
    "training_loop(\n",
    "    n_epochs=1,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=valid_loader,\n",
    "    patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7c04f-19a6-4268-9c94-9936d7d6431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "testing_set = ImageFolder(test_folder)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b162f26-bc0f-4f76-8db1-4f37f6de60f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "test_accuracy, test_loss, _ = validate(model, train_loader, test_loader, loss_fn, testing=True)\n",
    "\n",
    "print(f\"Average Loss: {test_loss / len(test_loader)}, Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc5cbf0-c0b0-4f6f-aefa-3e6bdf0c96ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        },\n",
    "        \"model_pth\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc1237-ad71-44d4-8da4-829cef443972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# load_model\n",
    "# loaded_model = ImageClassifier()\n",
    "# optimizer = torch.optim.Adam(loaded_model.parameters())  \n",
    "# checkpoint = torch.load('model_pth')\n",
    "\n",
    "# loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
